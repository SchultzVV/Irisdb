bundle:
  name: iris_bundle

variables:
  environment:
    description: "Environment name (dev/prod)"
    default: "dev"
  
  current_user:
    description: "Current user for permissions"
    default: "${workspace.current_user.userName}"
    
  source:
    description: "Fonte de dados do dataset Iris (via seaborn)"
    default: "seaborn://iris"
  output_bronze_table:
    description: "Tabela Unity Catalog para camada Bronze"
    default: "default.iris_bronze"
  output_silver_table:
    description: "Tabela Unity Catalog para camada Silver"
    default: "default.iris_silver"
  output_gold_table:
    description: "Tabela Unity Catalog para camada Gold"
    default: "default.iris_gold"
  output_model:
    description: "Nome do modelo MLflow no Unity Catalog"
    default: "default.iris_model"
  catalog_name:
    description: "Nome do cat√°logo Unity Catalog"
    default: "workspace"
  schema_name:
    description: "Nome do schema Unity Catalog"
    default: "default"
  notification_email:
    description: "Email para notifica√ß√µes dos jobs"
    default: "admin@company.com"
  teams_webhook:
    description: "Webhook do Microsoft Teams para alertas"
    default: "https://your-teams-webhook-url.com"
  
  elasticsearch_host:
    description: "Host do ElasticSearch para logs"
    default: "localhost"
  
  elasticsearch_port:
    description: "Porta do ElasticSearch"
    default: "9200"
  
  log4j_path:
    description: "Caminho para arquivo log4j.properties no Workspace"
    default: "/Workspace/Shared/iris_monitoring/log4j.properties"
  
  enable_monitoring:
    description: "Habilitar monitoramento avan√ßado"
    default: "true"
  # üñ•Ô∏è Configura√ß√µes de Cluster (opcional - atualmente usando Serverless)
  # Para usar clusters espec√≠ficos, descomente as vari√°veis abaixo:
  # cluster_id:
  #   description: "ID do cluster espec√≠fico para execu√ß√£o dos jobs"
  #   default: "${var.cluster_id}"  # Ou hardcode o ID do cluster
  # cluster_name:
  #   description: "Nome do cluster para identifica√ß√£o"
  #   default: "iris-processing-cluster"

sync:
  include:
    - notebooks/*
    - requirements.txt
    - tests/*

include:
  - resources/jobs/*.yml

targets:
  dev:
    mode: development
    default: true
    variables:
      environment: "dev"
      catalog_name: "workspace"
      schema_name: "default"
      # üñ•Ô∏è Cluster Config para DEV (comentado - usando Serverless)
      # cluster_id: "dev-cluster-id-here"
      # cluster_name: "iris-dev-cluster"
  
  prod:
    mode: production
    variables:
      environment: "prod"
      catalog_name: "main"
      schema_name: "iris_pipeline"
      output_bronze_table: "main.iris_pipeline.iris_bronze"
      output_silver_table: "main.iris_pipeline.iris_silver"
      output_gold_table: "main.iris_pipeline.iris_gold"
      output_model: "main.iris_pipeline.iris_model"
      # üñ•Ô∏è Cluster Config para PROD (comentado - usando Serverless)
      # cluster_id: "prod-cluster-id-here"
      # cluster_name: "iris-prod-cluster"

# üñ•Ô∏è CONFIGURA√á√ïES DE CLUSTER (comentado - atualmente usando Serverless)
# 
# Para usar clusters espec√≠ficos ao inv√©s de Serverless, descomente as se√ß√µes abaixo:
#
# resources:
#   clusters:
#     # Cluster para desenvolvimento
#     iris_dev_cluster:
#       cluster_name: "iris-dev-cluster"
#       spark_version: "13.3.x-scala2.12"
#       node_type_id: "i3.xlarge"
#       driver_node_type_id: "i3.xlarge"
#       num_workers: 2
#       autotermination_minutes: 30
#       spark_conf:
#         "spark.databricks.adaptive.enabled": "true"
#         "spark.databricks.adaptive.coalescePartitions.enabled": "true"
#       custom_tags:
#         Environment: "dev"
#         Project: "iris_pipeline"
#         Team: "data_engineering"
#     
#     # Cluster para produ√ß√£o
#     iris_prod_cluster:
#       cluster_name: "iris-prod-cluster"
#       spark_version: "13.3.x-scala2.12"
#       node_type_id: "i3.2xlarge"
#       driver_node_type_id: "i3.2xlarge"
#       num_workers: 4
#       autotermination_minutes: 60
#       spark_conf:
#         "spark.databricks.adaptive.enabled": "true"
#         "spark.databricks.adaptive.coalescePartitions.enabled": "true"
#         "spark.databricks.adaptive.localShuffleReader.enabled": "true"
#       custom_tags:
#         Environment: "prod"
#         Project: "iris_pipeline"
#         Team: "data_engineering"