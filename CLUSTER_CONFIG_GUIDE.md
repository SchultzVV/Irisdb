# ğŸ–¥ï¸ Guia de ConfiguraÃ§Ã£o de Clusters

## ğŸ“‹ SituaÃ§Ã£o Atual
- **Status**: Usando **Serverless Computing** (padrÃ£o Databricks)
- **Vantagens**: Sem gerenciamento de cluster, start rÃ¡pido, escalabilidade automÃ¡tica
- **Custos**: Pay-per-use, ideal para workloads esporÃ¡dicos

## ğŸ”§ Como Configurar Clusters EspecÃ­ficos

### 1ï¸âƒ£ **No arquivo `databricks.yml`** (ConfiguraÃ§Ã£o Global)

```yaml
variables:
  cluster_id:
    description: "ID do cluster especÃ­fico"
    default: "0123-456789-abcdef"

resources:
  clusters:
    iris_cluster:
      cluster_name: "iris-processing-cluster"
      spark_version: "13.3.x-scala2.12"
      node_type_id: "i3.xlarge"
      num_workers: 2
```

### 2ï¸âƒ£ **Nos arquivos de Job** (resources/jobs/*.yml)

#### OpÃ§Ã£o A: Cluster Existente
```yaml
jobs:
  my_job:
    existing_cluster_id: ${var.cluster_id}
```

#### OpÃ§Ã£o B: Novo Cluster por Job
```yaml
jobs:
  my_job:
    new_cluster:
      cluster_name: "job-specific-cluster"
      spark_version: "13.3.x-scala2.12"
      node_type_id: "i3.xlarge"
      num_workers: 2
```

#### OpÃ§Ã£o C: Instance Pool (Recomendado para ProduÃ§Ã£o)
```yaml
jobs:
  my_job:
    new_cluster:
      instance_pool_id: "pool-id-here"
      num_workers: 2
```

### 3ï¸âƒ£ **Level de Task** (Override especÃ­fico)
```yaml
tasks:
  - task_key: my_task
    existing_cluster_id: "task-specific-cluster"
```

## ğŸ¯ Quando Usar Cada OpÃ§Ã£o

| CenÃ¡rio | RecomendaÃ§Ã£o | Motivo |
|---------|-------------|---------|
| **Desenvolvimento** | Serverless | Flexibilidade, sem overhead |
| **ProduÃ§Ã£o Regular** | Cluster Pool | EficiÃªncia de custos |
| **Jobs CrÃ­ticos** | Cluster Dedicado | Performance garantida |
| **Workloads Grandes** | Auto Scaling Cluster | Escalabilidade |

## ğŸ’° ComparaÃ§Ã£o de Custos

- **Serverless**: $0.20/DBU + compute
- **Job Cluster**: $0.15/DBU + compute  
- **All-Purpose Cluster**: $0.40/DBU + compute
- **Instance Pool**: $0.15/DBU + compute (+ economia de start time)

## ğŸš€ Para Ativar Clusters no Futuro

1. **Descomente** as seÃ§Ãµes no `databricks.yml`
2. **Substitua** os IDs de cluster pelos valores reais
3. **Deploy** novamente: `make deploy`
4. **Teste** com um job: `make run_bronze`

## ğŸ“Š Monitoramento de Clusters

- **UI Databricks**: `/compute/clusters`
- **MÃ©tricas**: CPU, Memory, Network I/O
- **Logs**: Driver logs, Executor logs
- **Spark UI**: Jobs, Stages, Tasks

## âš¡ Dicas de Performance

```yaml
spark_conf:
  "spark.databricks.adaptive.enabled": "true"
  "spark.databricks.adaptive.coalescePartitions.enabled": "true"
  "spark.databricks.adaptive.localShuffleReader.enabled": "true"
  "spark.sql.adaptive.skewJoin.enabled": "true"
```

## ğŸ” Tags Recomendadas

```yaml
custom_tags:
  Environment: "dev|prod"
  Project: "iris_pipeline"
  Team: "data_engineering"
  CostCenter: "analytics"
  Owner: "vitor@company.com"
```
