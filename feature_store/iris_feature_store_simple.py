# Databricks notebook source
# MAGIC %md
# MAGIC # ğŸª Iris Feature Store Simples
# MAGIC 
# MAGIC Este notebook implementa um Feature Store simplificado para o dataset Iris.

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“¦ Imports e ConfiguraÃ§Ãµes

# COMMAND ----------

import pandas as pd
import numpy as np
from pyspark.sql import functions as F
from pyspark.sql.types import *
import mlflow
import seaborn as sns
from datetime import datetime

print("âœ… Bibliotecas importadas com sucesso!")
print(f"ğŸ•’ Timestamp: {datetime.now()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š Carregamento de Dados da Camada Silver

# COMMAND ----------

# Carregar dados da tabela Silver do Unity Catalog
silver_table = "hive_metastore.default.iris_silver"
print(f"ğŸ“Š Carregando dados da tabela Silver: {silver_table}")

try:
    df_silver = spark.table(silver_table)
    print("âœ… Tabela Silver carregada com sucesso!")
    
    # Verificar esquema e dados
    print("ğŸ“‹ Schema da tabela Silver:")
    df_silver.printSchema()
    
    # Adicionar ID Ãºnico para cada registro (para feature store)
    df_with_id = df_silver.withColumn("iris_id", F.monotonically_increasing_id())
    
    # Mostrar dados base
    print("ğŸ“Š Dados Silver carregados:")
    df_with_id.show(5)
    print(f"ğŸ“¦ Total de registros: {df_with_id.count()}")
    
except Exception as e:
    print(f"âŒ Erro ao carregar tabela Silver: {str(e)}")
    print("ğŸ”„ Usando dados do seaborn como fallback...")
    
    # Fallback para dados do seaborn
    df_iris = sns.load_dataset("iris")
    df_spark = spark.createDataFrame(df_iris)
    df_with_id = df_spark.withColumn("iris_id", F.monotonically_increasing_id())
    
    print("ğŸ“Š Dataset Iris (fallback) carregado:")
    df_with_id.show(5)
    print(f"ğŸ“¦ Total de registros: {df_with_id.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ› ï¸ Feature Engineering

# COMMAND ----------

def create_features(df):
    """
    Cria features engineered para o dataset Iris
    """
    print("ğŸ› ï¸ Criando features engineered...")
    
    # Features bÃ¡sicas
    df_features = df.withColumn(
        "sepal_area", F.col("sepal_length") * F.col("sepal_width")
    ).withColumn(
        "petal_area", F.col("petal_length") * F.col("petal_width")
    ).withColumn(
        "total_area", F.col("sepal_area") + F.col("petal_area")
    )
    
    # Features de proporÃ§Ã£o
    df_features = df_features.withColumn(
        "sepal_ratio", F.col("sepal_length") / F.col("sepal_width")
    ).withColumn(
        "petal_ratio", F.col("petal_length") / F.col("petal_width")
    ).withColumn(
        "area_ratio", F.col("sepal_area") / F.col("petal_area")
    )
    
    # Features de distÃ¢ncia euclidiana
    df_features = df_features.withColumn(
        "distance_from_origin",
        F.sqrt(
            F.pow(F.col("sepal_length"), 2) + 
            F.pow(F.col("sepal_width"), 2) + 
            F.pow(F.col("petal_length"), 2) + 
            F.pow(F.col("petal_width"), 2)
        )
    )
    
    # Timestamp para versionamento
    df_features = df_features.withColumn(
        "feature_timestamp", F.current_timestamp()
    )
    
    return df_features

# Aplicar feature engineering
df_feature_table = create_features(df_with_id)

print("ğŸ¯ Features criadas:")
df_feature_table.printSchema()
print(f"ğŸ“Š Total de colunas: {len(df_feature_table.columns)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ Salvar Feature Store

# COMMAND ----------

# Criar tabela temporÃ¡ria primeiro para testar
temp_table_name = "iris_features_temp"
df_feature_table.createOrReplaceTempView(temp_table_name)

print(f"âœ… Tabela temporÃ¡ria criada: {temp_table_name}")

# Testar consulta
result = spark.sql(f"SELECT COUNT(*) as count FROM {temp_table_name}")
record_count = result.collect()[0]['count']
print(f"ğŸ“Š Registros na tabela temporÃ¡ria: {record_count}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ—ƒï¸ Salvar como Tabela Delta

# COMMAND ----------

# Tentar salvar usando formato Delta simples
try:
    # Usar apenas o schema default sem especificar catÃ¡logo
    simple_table_name = "iris_features"
    
    print(f"ğŸ’¾ Salvando como tabela Delta: {simple_table_name}")
    
    df_feature_table.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .saveAsTable(simple_table_name)
    
    print(f"âœ… Tabela Delta criada: {simple_table_name}")
    
    # Validar criaÃ§Ã£o
    validation_df = spark.table(simple_table_name)
    print(f"ğŸ” ValidaÃ§Ã£o - Registros: {validation_df.count()}")
    
except Exception as e:
    print(f"âŒ Erro ao criar tabela Delta: {str(e)}")
    
    # Fallback para tabela temporÃ¡ria
    print("ğŸ”„ Usando tabela temporÃ¡ria como fallback")
    temp_table_name = "iris_features_temp"
    df_feature_table.createOrReplaceTempView(temp_table_name)
    print(f"âœ… Tabela temporÃ¡ria criada: {temp_table_name}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ˆ AnÃ¡lise das Features

# COMMAND ----------

# AnÃ¡lise estatÃ­stica das features numÃ©ricas
print("ğŸ“Š AnÃ¡lise EstatÃ­stica das Features:")

numeric_features = [
    "sepal_length", "sepal_width", "petal_length", "petal_width",
    "sepal_area", "petal_area", "total_area", 
    "sepal_ratio", "petal_ratio", "area_ratio", "distance_from_origin"
]

for feature in numeric_features:
    stats = df_feature_table.select(
        F.mean(feature).alias("mean"),
        F.stddev(feature).alias("std"),
        F.min(feature).alias("min"),
        F.max(feature).alias("max")
    ).collect()[0]
    
    print(f"ğŸ“Š {feature}:")
    print(f"   Mean: {stats['mean']:.3f}, Std: {stats['std']:.3f}")
    print(f"   Min: {stats['min']:.3f}, Max: {stats['max']:.3f}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ·ï¸ AnÃ¡lise por EspÃ©cie

# COMMAND ----------

# EstatÃ­sticas por espÃ©cie
print("ğŸ·ï¸ AnÃ¡lise por EspÃ©cie:")

species_stats = df_feature_table.groupBy("species").agg(
    F.count("*").alias("count"),
    F.mean("total_area").alias("avg_total_area"),
    F.mean("distance_from_origin").alias("avg_distance"),
    F.mean("sepal_ratio").alias("avg_sepal_ratio"),
    F.mean("petal_ratio").alias("avg_petal_ratio")
).collect()

for row in species_stats:
    print(f"ğŸŒ¸ {row['species']}:")
    print(f"   Count: {row['count']}")
    print(f"   Avg Total Area: {row['avg_total_area']:.3f}")
    print(f"   Avg Distance: {row['avg_distance']:.3f}")
    print(f"   Avg Sepal Ratio: {row['avg_sepal_ratio']:.3f}")
    print(f"   Avg Petal Ratio: {row['avg_petal_ratio']:.3f}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‹ Resumo do Feature Store

# COMMAND ----------

print("ğŸ¯ RESUMO DO FEATURE STORE")
print("=" * 50)
print(f"ğŸ“Š Total Features Created: {len(df_feature_table.columns)}")
print(f"ğŸ“¦ Total Records: {df_feature_table.count()}")
print(f"ğŸ·ï¸ Classes: {df_feature_table.select('species').distinct().count()}")

# Listar todas as features
feature_list = df_feature_table.columns
print(f"\nğŸ“ Features DisponÃ­veis ({len(feature_list)}):")
for i, feature in enumerate(feature_list, 1):
    print(f"   {i:2d}. {feature}")

print("\nâœ… Feature Store criado com sucesso!")
print("ğŸš€ Pronto para uso em modelos de ML!")
