resources:
  jobs:
    bronze_job:
      name: iris_bronze_ingestion
      # üñ•Ô∏è CONFIGURA√á√ÉO DE CLUSTER (comentado - atualmente usando Serverless)
      # 
      # Op√ß√£o 1: Usar cluster existente por ID
      # existing_cluster_id: ${var.cluster_id}
      #
      # Op√ß√£o 2: Criar cluster job-espec√≠fico
      # new_cluster:
      #   cluster_name: "bronze-job-cluster"
      #   spark_version: "13.3.x-scala2.12"
      #   node_type_id: "i3.xlarge"
      #   driver_node_type_id: "i3.xlarge"
      #   num_workers: 2
      #   autotermination_minutes: 30
      #   spark_conf:
      #     "spark.databricks.adaptive.enabled": "true"
      #     "spark.databricks.adaptive.coalescePartitions.enabled": "true"
      #   custom_tags:
      #     Job: "bronze_ingestion"
      #     Environment: "dev"
      #
      # Op√ß√£o 3: Usar cluster pool (mais eficiente para m√∫ltiplos jobs)
      # new_cluster:
      #   cluster_name: "bronze-job-from-pool"
      #   instance_pool_id: "pool-id-here"
      #   spark_version: "13.3.x-scala2.12"
      #   num_workers: 2
      #   autotermination_minutes: 30
      
      tasks:
        - task_key: ingest_bronze
          description: "Ingesta os dados brutos no Delta Lake (camada Bronze) usando Unity Catalog"
          notebook_task:
            notebook_path: /Workspace/Users/vitorvazschultz@gmail.com/.bundle/iris_bundle/dev/files/notebooks/01_ingest_bronze
            base_parameters:
              source: ${var.source}
              output_bronze_table: ${var.output_bronze_table}
              catalog_name: ${var.catalog_name}
              schema_name: ${var.schema_name}
          timeout_seconds: 3600
          max_retries: 1
          # üñ•Ô∏è Task-level cluster override (se necess√°rio diferente do job-level)
          # existing_cluster_id: "task-specific-cluster-id"