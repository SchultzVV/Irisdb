{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d92e99f",
   "metadata": {},
   "source": [
    "# ğŸ§ª Iris MLOps - Test Runner\n",
    "\n",
    "Este notebook executa todos os testes do projeto Iris MLOps para validar:\n",
    "- Qualidade dos dados\n",
    "- Funcionalidade dos pipelines\n",
    "- Integridade das tabelas\n",
    "\n",
    "## ğŸ“‹ Status dos Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a414214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessÃ¡rios\n",
    "import sys\n",
    "import os\n",
    "import pytest\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Configurar paths\n",
    "project_root = '/Workspace/Repos/iris_bundle'\n",
    "sys.path.append(project_root)\n",
    "sys.path.append(f'{project_root}/tests')\n",
    "\n",
    "print(f\"ğŸ“ Project root: {project_root}\")\n",
    "print(f\"ğŸ Python version: {sys.version}\")\n",
    "print(f\"â° Test execution time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e177b9d",
   "metadata": {},
   "source": [
    "## ğŸ”§ ConfiguraÃ§Ã£o do Ambiente de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Spark Session para testes\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IrisMLOps_TestRunner\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark Session criada: {spark.version}\")\n",
    "print(f\"ğŸ“Š CatÃ¡logo atual: {spark.catalog.currentDatabase()}\")\n",
    "\n",
    "# Verificar se as tabelas existem\n",
    "try:\n",
    "    tables = spark.catalog.listTables(\"default\")\n",
    "    iris_tables = [t.name for t in tables if 'iris' in t.name]\n",
    "    print(f\"ğŸ—‚ï¸ Tabelas Iris encontradas: {iris_tables}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Erro ao listar tabelas: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de4771",
   "metadata": {},
   "source": [
    "## ğŸ§ª Teste 1: ValidaÃ§Ã£o de Dados Mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do banco mock\n",
    "print(\"ğŸ” Executando teste do banco mock...\")\n",
    "\n",
    "try:\n",
    "    from tests.mock_db import create_mock_iris_db\n",
    "    \n",
    "    # Criar conexÃ£o mock\n",
    "    conn = create_mock_iris_db()\n",
    "    \n",
    "    # Verificar dados\n",
    "    df_mock = pd.read_sql_query(\"SELECT * FROM iris LIMIT 5\", conn)\n",
    "    \n",
    "    print(f\"âœ… Mock DB criado com sucesso!\")\n",
    "    print(f\"ğŸ“Š Registros no mock: {len(pd.read_sql_query('SELECT * FROM iris', conn))}\")\n",
    "    print(f\"ğŸ·ï¸ Colunas disponÃ­veis: {list(df_mock.columns)}\")\n",
    "    \n",
    "    # Mostrar amostra\n",
    "    display(df_mock)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro no teste mock: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b06d1",
   "metadata": {},
   "source": [
    "## ğŸ§ª Teste 2: ValidaÃ§Ã£o de Tabelas Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd576628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste das tabelas Unity Catalog\n",
    "print(\"ğŸ” Executando validaÃ§Ã£o das tabelas Unity Catalog...\")\n",
    "\n",
    "# Definir tabelas para testar\n",
    "tables_to_test = [\n",
    "    \"default.iris_bronze\",\n",
    "    \"default.iris_silver\", \n",
    "    \"default.iris_gold\"\n",
    "]\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for table_name in tables_to_test:\n",
    "    try:\n",
    "        print(f\"\\nğŸ“‹ Testando tabela: {table_name}\")\n",
    "        \n",
    "        # Verificar se a tabela existe\n",
    "        df = spark.table(table_name)\n",
    "        count = df.count()\n",
    "        \n",
    "        # Verificar schema\n",
    "        schema_info = [(field.name, field.dataType.simpleString()) for field in df.schema.fields]\n",
    "        \n",
    "        # Verificar nulls\n",
    "        null_counts = {}\n",
    "        for col in df.columns:\n",
    "            null_count = df.filter(F.col(col).isNull()).count()\n",
    "            null_counts[col] = null_count\n",
    "        \n",
    "        test_results[table_name] = {\n",
    "            \"status\": \"âœ… PASSOU\",\n",
    "            \"count\": count,\n",
    "            \"schema\": schema_info,\n",
    "            \"nulls\": null_counts\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… Registros: {count}\")\n",
    "        print(f\"  ğŸ“Š Colunas: {len(df.columns)}\")\n",
    "        print(f\"  ğŸš« Nulls totais: {sum(null_counts.values())}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_results[table_name] = {\n",
    "            \"status\": \"âŒ FALHOU\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        print(f\"  âŒ Erro: {e}\")\n",
    "\n",
    "# Mostrar resumo\n",
    "print(\"\\nğŸ“Š RESUMO DOS TESTES:\")\n",
    "for table, result in test_results.items():\n",
    "    print(f\"{table}: {result['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749094b4",
   "metadata": {},
   "source": [
    "## ğŸ§ª Teste 3: Qualidade de Dados - Camada Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste especÃ­fico da qualidade de dados Silver\n",
    "print(\"ğŸ” Executando teste de qualidade - Silver layer...\")\n",
    "\n",
    "try:\n",
    "    # Carregar tabela silver\n",
    "    df_silver = spark.table(\"default.iris_silver\")\n",
    "    \n",
    "    # Testes de qualidade\n",
    "    quality_tests = {\n",
    "        \"total_records\": df_silver.count(),\n",
    "        \"duplicate_check\": df_silver.count() - df_silver.dropDuplicates().count(),\n",
    "        \"null_check\": {},\n",
    "        \"data_types\": {},\n",
    "        \"value_ranges\": {}\n",
    "    }\n",
    "    \n",
    "    # Verificar nulls por coluna\n",
    "    for col in df_silver.columns:\n",
    "        null_count = df_silver.filter(F.col(col).isNull()).count()\n",
    "        quality_tests[\"null_check\"][col] = null_count\n",
    "    \n",
    "    # Verificar tipos de dados\n",
    "    for field in df_silver.schema.fields:\n",
    "        quality_tests[\"data_types\"][field.name] = field.dataType.simpleString()\n",
    "    \n",
    "    # Verificar ranges para colunas numÃ©ricas\n",
    "    numeric_cols = [f.name for f in df_silver.schema.fields if 'double' in f.dataType.simpleString()]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        stats = df_silver.select(\n",
    "            F.min(col).alias('min'),\n",
    "            F.max(col).alias('max'),\n",
    "            F.mean(col).alias('mean'),\n",
    "            F.stddev(col).alias('stddev')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        quality_tests[\"value_ranges\"][col] = {\n",
    "            \"min\": float(stats['min']),\n",
    "            \"max\": float(stats['max']),\n",
    "            \"mean\": float(stats['mean']),\n",
    "            \"stddev\": float(stats['stddev'])\n",
    "        }\n",
    "    \n",
    "    # Avaliar resultados\n",
    "    print(f\"\\nğŸ“Š RESULTADOS DE QUALIDADE:\")\n",
    "    print(f\"  ğŸ“ˆ Total de registros: {quality_tests['total_records']}\")\n",
    "    print(f\"  ğŸ” Duplicatas encontradas: {quality_tests['duplicate_check']}\")\n",
    "    print(f\"  ğŸš« Total de nulls: {sum(quality_tests['null_check'].values())}\")\n",
    "    \n",
    "    # Verificar se passou nos testes\n",
    "    quality_passed = (\n",
    "        quality_tests['total_records'] > 0 and\n",
    "        quality_tests['duplicate_check'] == 0 and\n",
    "        sum(quality_tests['null_check'].values()) == 0\n",
    "    )\n",
    "    \n",
    "    if quality_passed:\n",
    "        print(\"\\nâœ… TODOS OS TESTES DE QUALIDADE PASSARAM!\")\n",
    "    else:\n",
    "        print(\"\\nâŒ ALGUNS TESTES DE QUALIDADE FALHARAM!\")\n",
    "    \n",
    "    # Criar DataFrame com resultados para visualizaÃ§Ã£o\n",
    "    quality_df = pd.DataFrame([\n",
    "        {\"Teste\": \"Total de Registros\", \"Resultado\": quality_tests['total_records'], \"Status\": \"âœ…\" if quality_tests['total_records'] > 0 else \"âŒ\"},\n",
    "        {\"Teste\": \"Sem Duplicatas\", \"Resultado\": quality_tests['duplicate_check'], \"Status\": \"âœ…\" if quality_tests['duplicate_check'] == 0 else \"âŒ\"},\n",
    "        {\"Teste\": \"Sem Nulls\", \"Resultado\": sum(quality_tests['null_check'].values()), \"Status\": \"âœ…\" if sum(quality_tests['null_check'].values()) == 0 else \"âŒ\"}\n",
    "    ])\n",
    "    \n",
    "    display(quality_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro no teste de qualidade: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a59a3",
   "metadata": {},
   "source": [
    "## ğŸ§ª Teste 4: ExecuÃ§Ã£o dos Testes UnitÃ¡rios com Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75125922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar pytest nos arquivos de teste\n",
    "print(\"ğŸ” Executando testes unitÃ¡rios com pytest...\")\n",
    "\n",
    "try:\n",
    "    # Mudar para o diretÃ³rio do projeto\n",
    "    os.chdir('/Workspace/Repos/iris_bundle')\n",
    "    \n",
    "    # Executar pytest\n",
    "    result = subprocess.run(\n",
    "        ['python', '-m', 'pytest', 'tests/', '-v', '--tb=short'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“¤ Exit code: {result.returncode}\")\n",
    "    print(f\"\\nğŸ“‹ STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(f\"\\nâš ï¸ STDERR:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nâœ… TODOS OS TESTES UNITÃRIOS PASSARAM!\")\n",
    "    else:\n",
    "        print(\"\\nâŒ ALGUNS TESTES UNITÃRIOS FALHARAM!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro ao executar pytest: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b145c7",
   "metadata": {},
   "source": [
    "## ğŸ§ª Teste 5: VerificaÃ§Ã£o do Modelo MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do modelo MLflow\n",
    "print(\"ğŸ” Verificando modelo MLflow...\")\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    from mlflow import MlflowClient\n",
    "    \n",
    "    # Configurar MLflow\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    # Verificar se existe algum modelo registrado\n",
    "    try:\n",
    "        models = client.search_registered_models()\n",
    "        iris_models = [m for m in models if 'iris' in m.name.lower()]\n",
    "        \n",
    "        if iris_models:\n",
    "            print(f\"âœ… Modelos Iris encontrados: {len(iris_models)}\")\n",
    "            for model in iris_models:\n",
    "                print(f\"  ğŸ“¦ Modelo: {model.name}\")\n",
    "                latest_version = client.get_latest_versions(model.name, stages=[\"None\", \"Staging\", \"Production\"])\n",
    "                if latest_version:\n",
    "                    print(f\"    ğŸ·ï¸ VersÃ£o mais recente: {latest_version[0].version}\")\n",
    "                    print(f\"    ğŸ“Š Status: {latest_version[0].current_stage}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Nenhum modelo Iris encontrado no registry\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Registry nÃ£o disponÃ­vel, testando fallback: {e}\")\n",
    "        \n",
    "        # Testar carregamento de modelo via runs\n",
    "        experiments = client.search_experiments()\n",
    "        print(f\"ğŸ“Š Experimentos encontrados: {len(experiments)}\")\n",
    "        \n",
    "        # Buscar runs recentes\n",
    "        if experiments:\n",
    "            runs = client.search_runs(experiment_ids=[exp.experiment_id for exp in experiments[:3]])\n",
    "            model_runs = [run for run in runs if any('model' in tag for tag in run.data.tags.keys())]\n",
    "            print(f\"ğŸ¯ Runs com modelos: {len(model_runs)}\")\n",
    "            \n",
    "            if model_runs:\n",
    "                print(\"âœ… Fallback de modelo disponÃ­vel\")\n",
    "            else:\n",
    "                print(\"âš ï¸ Nenhum modelo encontrado em runs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro na verificaÃ§Ã£o do MLflow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76afba7",
   "metadata": {},
   "source": [
    "## ğŸ“Š RelatÃ³rio Final dos Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar relatÃ³rio final\n",
    "print(\"ğŸ“‹ RELATÃ“RIO FINAL DE TESTES - IRIS MLOPS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"â° Executado em: {datetime.now()}\")\n",
    "print(f\"ğŸ–¥ï¸ Ambiente: Databricks Workspace\")\n",
    "print(f\"ğŸ“¦ Spark Version: {spark.version}\")\n",
    "print()\n",
    "\n",
    "# Resumo dos componentes testados\n",
    "components = [\n",
    "    \"ğŸ—ƒï¸ Mock Database\",\n",
    "    \"ğŸ“Š Unity Catalog Tables\", \n",
    "    \"ğŸ” Data Quality (Silver)\",\n",
    "    \"ğŸ§ª Unit Tests (Pytest)\",\n",
    "    \"ğŸ¤– MLflow Models\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª COMPONENTES TESTADOS:\")\n",
    "for i, component in enumerate(components, 1):\n",
    "    print(f\"  {i}. {component}\")\n",
    "\n",
    "print()\n",
    "print(\"âœ… PRÃ“XIMOS PASSOS:\")\n",
    "print(\"  1. ğŸš€ Deploy via CI/CD: git push para main\")\n",
    "print(\"  2. ğŸ“Š Executar pipeline: make run-pipeline\")\n",
    "print(\"  3. ğŸ”„ Monitorar jobs no Databricks Workflows\")\n",
    "print()\n",
    "print(\"ğŸ’¡ DICA: Execute este notebook antes de cada deploy para garantir qualidade!\")\n",
    "\n",
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"\\nğŸ§¹ Spark session finalizada.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
