{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d92e99f",
   "metadata": {},
   "source": [
    "# 🧪 Iris MLOps - Test Runner\n",
    "\n",
    "Este notebook executa todos os testes do projeto Iris MLOps para validar:\n",
    "- Qualidade dos dados\n",
    "- Funcionalidade dos pipelines\n",
    "- Integridade das tabelas\n",
    "\n",
    "## 📋 Status dos Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a414214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessários\n",
    "import sys\n",
    "import os\n",
    "import pytest\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Configurar paths\n",
    "project_root = '/Workspace/Repos/iris_bundle'\n",
    "sys.path.append(project_root)\n",
    "sys.path.append(f'{project_root}/tests')\n",
    "\n",
    "print(f\"📁 Project root: {project_root}\")\n",
    "print(f\"🐍 Python version: {sys.version}\")\n",
    "print(f\"⏰ Test execution time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e177b9d",
   "metadata": {},
   "source": [
    "## 🔧 Configuração do Ambiente de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Spark Session para testes\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IrisMLOps_TestRunner\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✅ Spark Session criada: {spark.version}\")\n",
    "print(f\"📊 Catálogo atual: {spark.catalog.currentDatabase()}\")\n",
    "\n",
    "# Verificar se as tabelas existem\n",
    "try:\n",
    "    tables = spark.catalog.listTables(\"default\")\n",
    "    iris_tables = [t.name for t in tables if 'iris' in t.name]\n",
    "    print(f\"🗂️ Tabelas Iris encontradas: {iris_tables}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erro ao listar tabelas: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de4771",
   "metadata": {},
   "source": [
    "## 🧪 Teste 1: Validação de Dados Mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do banco mock\n",
    "print(\"🔍 Executando teste do banco mock...\")\n",
    "\n",
    "try:\n",
    "    from tests.mock_db import create_mock_iris_db\n",
    "    \n",
    "    # Criar conexão mock\n",
    "    conn = create_mock_iris_db()\n",
    "    \n",
    "    # Verificar dados\n",
    "    df_mock = pd.read_sql_query(\"SELECT * FROM iris LIMIT 5\", conn)\n",
    "    \n",
    "    print(f\"✅ Mock DB criado com sucesso!\")\n",
    "    print(f\"📊 Registros no mock: {len(pd.read_sql_query('SELECT * FROM iris', conn))}\")\n",
    "    print(f\"🏷️ Colunas disponíveis: {list(df_mock.columns)}\")\n",
    "    \n",
    "    # Mostrar amostra\n",
    "    display(df_mock)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro no teste mock: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b06d1",
   "metadata": {},
   "source": [
    "## 🧪 Teste 2: Validação de Tabelas Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd576628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste das tabelas Unity Catalog\n",
    "print(\"🔍 Executando validação das tabelas Unity Catalog...\")\n",
    "\n",
    "# Definir tabelas para testar\n",
    "tables_to_test = [\n",
    "    \"default.iris_bronze\",\n",
    "    \"default.iris_silver\", \n",
    "    \"default.iris_gold\"\n",
    "]\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for table_name in tables_to_test:\n",
    "    try:\n",
    "        print(f\"\\n📋 Testando tabela: {table_name}\")\n",
    "        \n",
    "        # Verificar se a tabela existe\n",
    "        df = spark.table(table_name)\n",
    "        count = df.count()\n",
    "        \n",
    "        # Verificar schema\n",
    "        schema_info = [(field.name, field.dataType.simpleString()) for field in df.schema.fields]\n",
    "        \n",
    "        # Verificar nulls\n",
    "        null_counts = {}\n",
    "        for col in df.columns:\n",
    "            null_count = df.filter(F.col(col).isNull()).count()\n",
    "            null_counts[col] = null_count\n",
    "        \n",
    "        test_results[table_name] = {\n",
    "            \"status\": \"✅ PASSOU\",\n",
    "            \"count\": count,\n",
    "            \"schema\": schema_info,\n",
    "            \"nulls\": null_counts\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ Registros: {count}\")\n",
    "        print(f\"  📊 Colunas: {len(df.columns)}\")\n",
    "        print(f\"  🚫 Nulls totais: {sum(null_counts.values())}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_results[table_name] = {\n",
    "            \"status\": \"❌ FALHOU\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        print(f\"  ❌ Erro: {e}\")\n",
    "\n",
    "# Mostrar resumo\n",
    "print(\"\\n📊 RESUMO DOS TESTES:\")\n",
    "for table, result in test_results.items():\n",
    "    print(f\"{table}: {result['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749094b4",
   "metadata": {},
   "source": [
    "## 🧪 Teste 3: Qualidade de Dados - Camada Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste específico da qualidade de dados Silver\n",
    "print(\"🔍 Executando teste de qualidade - Silver layer...\")\n",
    "\n",
    "try:\n",
    "    # Carregar tabela silver\n",
    "    df_silver = spark.table(\"default.iris_silver\")\n",
    "    \n",
    "    # Testes de qualidade\n",
    "    quality_tests = {\n",
    "        \"total_records\": df_silver.count(),\n",
    "        \"duplicate_check\": df_silver.count() - df_silver.dropDuplicates().count(),\n",
    "        \"null_check\": {},\n",
    "        \"data_types\": {},\n",
    "        \"value_ranges\": {}\n",
    "    }\n",
    "    \n",
    "    # Verificar nulls por coluna\n",
    "    for col in df_silver.columns:\n",
    "        null_count = df_silver.filter(F.col(col).isNull()).count()\n",
    "        quality_tests[\"null_check\"][col] = null_count\n",
    "    \n",
    "    # Verificar tipos de dados\n",
    "    for field in df_silver.schema.fields:\n",
    "        quality_tests[\"data_types\"][field.name] = field.dataType.simpleString()\n",
    "    \n",
    "    # Verificar ranges para colunas numéricas\n",
    "    numeric_cols = [f.name for f in df_silver.schema.fields if 'double' in f.dataType.simpleString()]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        stats = df_silver.select(\n",
    "            F.min(col).alias('min'),\n",
    "            F.max(col).alias('max'),\n",
    "            F.mean(col).alias('mean'),\n",
    "            F.stddev(col).alias('stddev')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        quality_tests[\"value_ranges\"][col] = {\n",
    "            \"min\": float(stats['min']),\n",
    "            \"max\": float(stats['max']),\n",
    "            \"mean\": float(stats['mean']),\n",
    "            \"stddev\": float(stats['stddev'])\n",
    "        }\n",
    "    \n",
    "    # Avaliar resultados\n",
    "    print(f\"\\n📊 RESULTADOS DE QUALIDADE:\")\n",
    "    print(f\"  📈 Total de registros: {quality_tests['total_records']}\")\n",
    "    print(f\"  🔍 Duplicatas encontradas: {quality_tests['duplicate_check']}\")\n",
    "    print(f\"  🚫 Total de nulls: {sum(quality_tests['null_check'].values())}\")\n",
    "    \n",
    "    # Verificar se passou nos testes\n",
    "    quality_passed = (\n",
    "        quality_tests['total_records'] > 0 and\n",
    "        quality_tests['duplicate_check'] == 0 and\n",
    "        sum(quality_tests['null_check'].values()) == 0\n",
    "    )\n",
    "    \n",
    "    if quality_passed:\n",
    "        print(\"\\n✅ TODOS OS TESTES DE QUALIDADE PASSARAM!\")\n",
    "    else:\n",
    "        print(\"\\n❌ ALGUNS TESTES DE QUALIDADE FALHARAM!\")\n",
    "    \n",
    "    # Criar DataFrame com resultados para visualização\n",
    "    quality_df = pd.DataFrame([\n",
    "        {\"Teste\": \"Total de Registros\", \"Resultado\": quality_tests['total_records'], \"Status\": \"✅\" if quality_tests['total_records'] > 0 else \"❌\"},\n",
    "        {\"Teste\": \"Sem Duplicatas\", \"Resultado\": quality_tests['duplicate_check'], \"Status\": \"✅\" if quality_tests['duplicate_check'] == 0 else \"❌\"},\n",
    "        {\"Teste\": \"Sem Nulls\", \"Resultado\": sum(quality_tests['null_check'].values()), \"Status\": \"✅\" if sum(quality_tests['null_check'].values()) == 0 else \"❌\"}\n",
    "    ])\n",
    "    \n",
    "    display(quality_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro no teste de qualidade: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a59a3",
   "metadata": {},
   "source": [
    "## 🧪 Teste 4: Execução dos Testes Unitários com Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75125922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar pytest nos arquivos de teste\n",
    "print(\"🔍 Executando testes unitários com pytest...\")\n",
    "\n",
    "try:\n",
    "    # Mudar para o diretório do projeto\n",
    "    os.chdir('/Workspace/Repos/iris_bundle')\n",
    "    \n",
    "    # Executar pytest\n",
    "    result = subprocess.run(\n",
    "        ['python', '-m', 'pytest', 'tests/', '-v', '--tb=short'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(f\"📤 Exit code: {result.returncode}\")\n",
    "    print(f\"\\n📋 STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(f\"\\n⚠️ STDERR:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n✅ TODOS OS TESTES UNITÁRIOS PASSARAM!\")\n",
    "    else:\n",
    "        print(\"\\n❌ ALGUNS TESTES UNITÁRIOS FALHARAM!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao executar pytest: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b145c7",
   "metadata": {},
   "source": [
    "## 🧪 Teste 5: Verificação do Modelo MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do modelo MLflow\n",
    "print(\"🔍 Verificando modelo MLflow...\")\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    from mlflow import MlflowClient\n",
    "    \n",
    "    # Configurar MLflow\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    # Verificar se existe algum modelo registrado\n",
    "    try:\n",
    "        models = client.search_registered_models()\n",
    "        iris_models = [m for m in models if 'iris' in m.name.lower()]\n",
    "        \n",
    "        if iris_models:\n",
    "            print(f\"✅ Modelos Iris encontrados: {len(iris_models)}\")\n",
    "            for model in iris_models:\n",
    "                print(f\"  📦 Modelo: {model.name}\")\n",
    "                latest_version = client.get_latest_versions(model.name, stages=[\"None\", \"Staging\", \"Production\"])\n",
    "                if latest_version:\n",
    "                    print(f\"    🏷️ Versão mais recente: {latest_version[0].version}\")\n",
    "                    print(f\"    📊 Status: {latest_version[0].current_stage}\")\n",
    "        else:\n",
    "            print(\"⚠️ Nenhum modelo Iris encontrado no registry\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Registry não disponível, testando fallback: {e}\")\n",
    "        \n",
    "        # Testar carregamento de modelo via runs\n",
    "        experiments = client.search_experiments()\n",
    "        print(f\"📊 Experimentos encontrados: {len(experiments)}\")\n",
    "        \n",
    "        # Buscar runs recentes\n",
    "        if experiments:\n",
    "            runs = client.search_runs(experiment_ids=[exp.experiment_id for exp in experiments[:3]])\n",
    "            model_runs = [run for run in runs if any('model' in tag for tag in run.data.tags.keys())]\n",
    "            print(f\"🎯 Runs com modelos: {len(model_runs)}\")\n",
    "            \n",
    "            if model_runs:\n",
    "                print(\"✅ Fallback de modelo disponível\")\n",
    "            else:\n",
    "                print(\"⚠️ Nenhum modelo encontrado em runs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro na verificação do MLflow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76afba7",
   "metadata": {},
   "source": [
    "## 📊 Relatório Final dos Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar relatório final\n",
    "print(\"📋 RELATÓRIO FINAL DE TESTES - IRIS MLOPS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"⏰ Executado em: {datetime.now()}\")\n",
    "print(f\"🖥️ Ambiente: Databricks Workspace\")\n",
    "print(f\"📦 Spark Version: {spark.version}\")\n",
    "print()\n",
    "\n",
    "# Resumo dos componentes testados\n",
    "components = [\n",
    "    \"🗃️ Mock Database\",\n",
    "    \"📊 Unity Catalog Tables\", \n",
    "    \"🔍 Data Quality (Silver)\",\n",
    "    \"🧪 Unit Tests (Pytest)\",\n",
    "    \"🤖 MLflow Models\"\n",
    "]\n",
    "\n",
    "print(\"🧪 COMPONENTES TESTADOS:\")\n",
    "for i, component in enumerate(components, 1):\n",
    "    print(f\"  {i}. {component}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ PRÓXIMOS PASSOS:\")\n",
    "print(\"  1. 🚀 Deploy via CI/CD: git push para main\")\n",
    "print(\"  2. 📊 Executar pipeline: make run-pipeline\")\n",
    "print(\"  3. 🔄 Monitorar jobs no Databricks Workflows\")\n",
    "print()\n",
    "print(\"💡 DICA: Execute este notebook antes de cada deploy para garantir qualidade!\")\n",
    "\n",
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"\\n🧹 Spark session finalizada.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
