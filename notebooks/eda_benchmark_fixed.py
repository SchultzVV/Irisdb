# Databricks notebook source
# MAGIC %md
# MAGIC # ğŸ“Š EDA & Benchmark - Feature Store Analysis
# MAGIC 
# MAGIC Este notebook realiza anÃ¡lise exploratÃ³ria das features do Feature Store
# MAGIC e executa benchmark de mÃºltiplos modelos de Machine Learning.

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“¦ Imports e ConfiguraÃ§Ãµes

# COMMAND ----------

import pandas as pd
import numpy as np
from pyspark.sql import functions as F
from pyspark.sql.types import *
import mlflow
import mlflow.sklearn
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

print("âœ… Bibliotecas importadas com sucesso!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š Carregamento dos Dados da Feature Store

# COMMAND ----------

# Carregar dados da Feature Store
feature_table_name = "iris_features"  # Tabela criada pelo Feature Store

try:
    features_df = spark.table(feature_table_name)
    print(f"âœ… Feature Store carregada: {feature_table_name}")
    print(f"ğŸ“Š Total de registros: {features_df.count()}")
    print(f"ğŸ“Š Total de features: {len(features_df.columns)}")
    
    # Mostrar schema
    print("\nğŸ“‹ Schema das Features:")
    features_df.printSchema()
    
    # Converter para Pandas para anÃ¡lises
    features_pdf = features_df.toPandas()
    print(f"âœ… Dados convertidos para Pandas: {features_pdf.shape}")
    
except Exception as e:
    print(f"âŒ Erro ao carregar Feature Store: {str(e)}")
    
    # Fallback: tentar outras tabelas disponÃ­veis
    print("\nğŸ” Verificando tabelas disponÃ­veis...")
    available_tables = spark.sql("SHOW TABLES").collect()
    for table in available_tables:
        print(f"   - {table['namespace']}.{table['tableName']}")
    
    # Tentar usar tabela Silver como fallback
    fallback_table = "iris_silver"
    print(f"\nğŸ”„ Tentando usar tabela Silver: {fallback_table}")
    features_df = spark.table(fallback_table)
    features_pdf = features_df.toPandas()
    print(f"âœ… Fallback bem-sucedido: {features_pdf.shape}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” AnÃ¡lise ExploratÃ³ria de Dados (EDA)

# COMMAND ----------

print("ğŸ” ANÃLISE EXPLORATÃ“RIA DE DADOS")
print("=" * 50)

# InformaÃ§Ãµes bÃ¡sicas do dataset
print(f"ğŸ“Š DimensÃµes do dataset: {features_pdf.shape}")
print(f"ğŸ“‹ Colunas: {list(features_pdf.columns)}")
print(f"ğŸ·ï¸ Tipos de dados:")
for col in features_pdf.columns:
    print(f"   {col}: {features_pdf[col].dtype}")

# Verificar valores nulos
print(f"\nâ“ Valores nulos por coluna:")
null_counts = features_pdf.isnull().sum()
for col, count in null_counts.items():
    if count > 0:
        print(f"   {col}: {count}")
    
if null_counts.sum() == 0:
    print("   âœ… Nenhum valor nulo encontrado!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ˆ EstatÃ­sticas Descritivas

# COMMAND ----------

print("ğŸ“ˆ ESTATÃSTICAS DESCRITIVAS")
print("=" * 50)

# Selecionar apenas colunas numÃ©ricas (excluindo species e timestamp)
numeric_cols = features_pdf.select_dtypes(include=[np.number]).columns.tolist()
if 'iris_id' in numeric_cols:
    numeric_cols.remove('iris_id')

print(f"ğŸ“Š Features numÃ©ricas analisadas: {len(numeric_cols)}")
for col in numeric_cols:
    print(f"   - {col}")

# EstatÃ­sticas descritivas
stats_df = features_pdf[numeric_cols].describe()
display(stats_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ·ï¸ AnÃ¡lise por EspÃ©cie

# COMMAND ----------

print("ğŸ·ï¸ ANÃLISE POR ESPÃ‰CIE")
print("=" * 50)

# Verificar se a coluna species existe
if 'species' in features_pdf.columns:
    # Contagem por espÃ©cie
    species_counts = features_pdf['species'].value_counts()
    print("ğŸ“Š DistribuiÃ§Ã£o por espÃ©cie:")
    for species, count in species_counts.items():
        print(f"   {species}: {count} registros")
    
    # EstatÃ­sticas por espÃ©cie
    print("\nğŸ“ˆ MÃ©dias por espÃ©cie:")
    # Selecionar principais features originais para anÃ¡lise
    main_features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
    available_features = [col for col in main_features if col in features_pdf.columns]
    
    if available_features:
        class_means = features_pdf.groupby('species')[available_features].mean()
        display(class_means)
    else:
        print("   âš ï¸ Features principais nÃ£o encontradas")
        # Usar as primeiras 4 colunas numÃ©ricas
        if len(numeric_cols) >= 4:
            class_means = features_pdf.groupby('species')[numeric_cols[:4]].mean()
            display(class_means)
else:
    print("   âš ï¸ Coluna 'species' nÃ£o encontrada no dataset")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– PreparaÃ§Ã£o dos Dados para ML

# COMMAND ----------

print("ğŸ¤– PREPARAÃ‡ÃƒO DOS DADOS PARA MACHINE LEARNING")
print("=" * 50)

# Preparar features e target
if 'species' in features_pdf.columns:
    # Separar features e target
    X = features_pdf[numeric_cols]
    y = features_pdf['species']
    
    print(f"ğŸ“Š Features shape: {X.shape}")
    print(f"ğŸ·ï¸ Target shape: {y.shape}")
    print(f"ğŸ“‹ Classes Ãºnicas: {list(y.unique())}")
    
    # Split dos dados
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print(f"ğŸ“Š Treino: {X_train.shape[0]} registros")
    print(f"ğŸ“Š Teste: {X_test.shape[0]} registros")
    
    # NormalizaÃ§Ã£o dos dados
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print("âœ… Dados normalizados com StandardScaler")
    
else:
    print("âŒ NÃ£o foi possÃ­vel preparar os dados - coluna 'species' nÃ£o encontrada")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ† Benchmark de Modelos

# COMMAND ----------

print("ğŸ† BENCHMARK DE MODELOS DE MACHINE LEARNING")
print("=" * 50)

if 'species' in features_pdf.columns:
    # Configurar modelos para benchmark
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'SVM': SVC(random_state=42),
        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
        'Naive Bayes': GaussianNB()
    }
    
    # Resultados do benchmark
    results = []
    
    for model_name, model in models.items():
        print(f"\nğŸ”„ Treinando {model_name}...")
        with mlflow.start_run(run_name="model_monitoring_testing") as run:

        # with mlflow.start_run(run_name=f"eda_benchmark_{model_name.replace(' ', '_').lower()}"):
            # Cross-validation
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            # Treinar no conjunto completo de treino
            model.fit(X_train_scaled, y_train)
            
            # PrediÃ§Ãµes
            y_pred = model.predict(X_test_scaled)
            test_accuracy = accuracy_score(y_test, y_pred)
            
            # Log das mÃ©tricas no MLflow
            mlflow.log_metric("cv_accuracy_mean", cv_mean)
            mlflow.log_metric("cv_accuracy_std", cv_std)
            mlflow.log_metric("test_accuracy", test_accuracy)
            mlflow.log_param("model_type", model_name)
            mlflow.log_param("feature_count", X_train.shape[1])
            
            # Salvar modelo
            mlflow.sklearn.log_model(model, "model")
            
            # Armazenar resultados
            results.append({
                'Modelo': model_name,
                'CV Accuracy (Mean)': f"{cv_mean:.4f}",
                'CV Accuracy (Std)': f"{cv_std:.4f}",
                'Test Accuracy': f"{test_accuracy:.4f}",
                'CV Score Range': f"{cv_mean - cv_std:.4f} - {cv_mean + cv_std:.4f}"
            })
            
            print(f"   âœ… CV Accuracy: {cv_mean:.4f} Â± {cv_std:.4f}")
            print(f"   âœ… Test Accuracy: {test_accuracy:.4f}")
    
    # Mostrar resultados finais
    print("\nğŸ† RESULTADOS FINAIS DO BENCHMARK")
    print("=" * 60)
    
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('Test Accuracy', ascending=False)
    
    display(results_df)
    
    # Identificar melhor modelo
    best_model = results_df.iloc[0]
    print(f"\nğŸ¥‡ MELHOR MODELO: {best_model['Modelo']}")
    print(f"   ğŸ“Š Test Accuracy: {best_model['Test Accuracy']}")
    print(f"   ğŸ“Š CV Accuracy: {best_model['CV Accuracy (Mean)']} Â± {best_model['CV Accuracy (Std)']}")
    
else:
    print("âŒ NÃ£o foi possÃ­vel executar benchmark - dados nÃ£o disponÃ­veis")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‹ Resumo da AnÃ¡lise

# COMMAND ----------

print("ğŸ“‹ RESUMO DA ANÃLISE EDA & BENCHMARK")
print("=" * 50)

if 'species' in features_pdf.columns:
    print(f"âœ… Dataset: {features_pdf.shape[0]} registros, {len(numeric_cols)} features numÃ©ricas")
    print(f"âœ… Classes: {len(features_pdf['species'].unique())} espÃ©cies de Iris")
    print(f"âœ… Qualidade: Sem valores nulos")
    print(f"âœ… Modelos testados: {len(models)}")
    print(f"âœ… Melhor modelo: {best_model['Modelo']} ({best_model['Test Accuracy']} accuracy)")
    
    print("\nğŸ¯ CONCLUSÃ•ES:")
    print("   â€¢ Dataset bem balanceado e sem problemas de qualidade")
    print("   â€¢ MÃºltiplos modelos alcanÃ§aram alta performance")
    print("   â€¢ Features engineered da Feature Store sÃ£o efetivas")
    print("   â€¢ Pipeline MLOps pronto para produÃ§Ã£o")
    
else:
    print("âš ï¸ AnÃ¡lise limitada devido Ã  estrutura dos dados")
    print(f"âœ… Dataset carregado: {features_pdf.shape}")
    print(f"âœ… Features numÃ©ricas identificadas: {len(numeric_cols)}")

print("\nğŸš€ Feature Store & EDA Analysis completa!")
