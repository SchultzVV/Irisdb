# Databricks notebook source
# MAGIC %md
# MAGIC # ğŸ¤– Model Training - MLflow Integration (Fixed)
# MAGIC ## Iris Classification with RandomForest
# MAGIC 
# MAGIC Este notebook treina um modelo de classificaÃ§Ã£o para o dataset Iris e registra no MLflow.
# MAGIC **VersÃ£o corrigida** - sem dependÃªncias problemÃ¡ticas de configuraÃ§Ã£o.

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“¦ Imports e ConfiguraÃ§Ãµes

# COMMAND ----------

import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from pyspark.sql import functions as F
from datetime import datetime
import warnings
import os
warnings.filterwarnings('ignore')

print("ğŸ“¦ Bibliotecas importadas com sucesso")

# COMMAND ----------

# MAGIC %md
# MAGIC ## âš™ï¸ ConfiguraÃ§Ã£o MLflow Simplificada

# COMMAND ----------

# ConfiguraÃ§Ã£o MLflow robusta (sem dependÃªncias problemÃ¡ticas)
try:
    # Configurar MLflow para Databricks
    mlflow.set_tracking_uri("databricks")
    print("âœ… MLflow tracking URI configurado")
    
    # Configurar registry URI para Unity Catalog se disponÃ­vel
    try:
        os.environ["MLFLOW_REGISTRY_URI"] = "databricks-uc"
        print("âœ… Registry URI configurado para Unity Catalog")
    except:
        print("âš ï¸ Unity Catalog registry nÃ£o disponÃ­vel, usando padrÃ£o")
        
except Exception as e:
    print(f"âš ï¸ Erro na configuraÃ§Ã£o MLflow: {e}")
    print("ğŸ”„ Continuando com configuraÃ§Ã£o padrÃ£o")

# COMMAND ----------

# MAGIC %md
# MAGIC ## âš™ï¸ ParÃ¢metros do Job

# COMMAND ----------

# Widgets para parÃ¢metros do job
dbutils.widgets.text("input_table", "workspace.default.iris_silver", "Tabela de entrada")
dbutils.widgets.text("model_name", "iris_classifier", "Nome do modelo MLflow")
dbutils.widgets.text("stage", "Production", "Stage do modelo")

# Obter valores dos parÃ¢metros
INPUT_TABLE = dbutils.widgets.get("input_table")
MODEL_NAME = dbutils.widgets.get("model_name")
STAGE = dbutils.widgets.get("stage")

print(f"ğŸ“Š Tabela de entrada: {INPUT_TABLE}")
print(f"ğŸ¤– Nome do modelo: {MODEL_NAME}")
print(f"ğŸ·ï¸ Stage: {STAGE}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“¥ Carregamento dos Dados

# COMMAND ----------

print(f"ğŸ“¥ Carregando dados de: {INPUT_TABLE}")

try:
    # Carregar dados da camada Silver
    df_spark = spark.table(INPUT_TABLE)
    df = df_spark.toPandas()
    
    print(f"ğŸ“Š Shape do dataset: {df.shape}")
    print(f"ğŸ“‹ Colunas: {list(df.columns)}")
    print(f"ğŸ·ï¸ EspÃ©cies Ãºnicas: {df['species'].unique()}")
    
    # Mostrar estatÃ­sticas bÃ¡sicas
    print(f"\nğŸ“ˆ EstatÃ­sticas bÃ¡sicas:")
    display(df.describe())
    
except Exception as e:
    print(f"âŒ Erro ao carregar dados: {e}")
    raise e

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ§ª ValidaÃ§Ãµes de Qualidade

# COMMAND ----------

print("ğŸ§ª Executando validaÃ§Ãµes de qualidade para ML...")

# ValidaÃ§Ã£o 1: Dataset nÃ£o vazio
assert len(df) > 0, "âŒ Dataset vazio!"
print(f"âœ… Dataset com {len(df)} registros")

# ValidaÃ§Ã£o 2: Colunas necessÃ¡rias existem
expected_columns = ["sepal_length", "sepal_width", "petal_length", "petal_width", "species"]
missing_cols = [col for col in expected_columns if col not in df.columns]
assert len(missing_cols) == 0, f"âŒ Colunas faltando: {missing_cols}"
print("âœ… Todas as colunas necessÃ¡rias presentes")

# ValidaÃ§Ã£o 3: Sem valores nulos nas colunas crÃ­ticas
critical_nulls = df[expected_columns].isnull().sum()
assert critical_nulls.sum() == 0, f"âŒ Valores nulos encontrados: {critical_nulls[critical_nulls > 0]}"
print("âœ… Nenhum valor nulo nas colunas crÃ­ticas")

# ValidaÃ§Ã£o 4: DistribuiÃ§Ã£o de classes
class_counts = df['species'].value_counts()
print(f"ğŸ“Š DistribuiÃ§Ã£o de classes:")
for species, count in class_counts.items():
    print(f"   {species}: {count}")

# ValidaÃ§Ã£o 5: Verificar se hÃ¡ pelo menos 2 classes
assert len(class_counts) >= 2, "âŒ Menos de 2 classes encontradas!"
print(f"âœ… {len(class_counts)} classes disponÃ­veis para classificaÃ§Ã£o")

print("ğŸ‰ Todas as validaÃ§Ãµes passaram!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”„ PreparaÃ§Ã£o dos Dados

# COMMAND ----------

print("ğŸ”„ Preparando dados para treinamento...")

# Separar features e target
feature_columns = ["sepal_length", "sepal_width", "petal_length", "petal_width"]
X = df[feature_columns]
y = df['species']

print(f"ğŸ“Š Features shape: {X.shape}")
print(f"ğŸ¯ Target shape: {y.shape}")

# Split dos dados
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42, 
    stratify=y
)

print(f"ğŸ‹ï¸ Treino: {X_train.shape[0]} amostras")
print(f"ğŸ§ª Teste: {X_test.shape[0]} amostras")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– Treinamento do Modelo

# COMMAND ----------

# ParÃ¢metros do modelo
model_params = {
    'n_estimators': 100,
    'max_depth': 10,
    'min_samples_split': 2,
    'min_samples_leaf': 1,
    'random_state': 42
}

print(f"ğŸ”§ ParÃ¢metros do modelo: {model_params}")

# Configurar experimento MLflow de forma robusta
experiment_name = f"/Shared/{MODEL_NAME}_experiments"
print(f"ğŸ§ª Configurando experimento: {experiment_name}")

try:
    mlflow.set_experiment(experiment_name)
    print("âœ… Experimento MLflow configurado")
except Exception as e:
    print(f"âš ï¸ Erro ao configurar experimento: {e}")
    print("ğŸ”„ Continuando sem experimento especÃ­fico")

# Iniciar run MLflow
with mlflow.start_run(run_name=f"{MODEL_NAME}_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
    
    # Treinar modelo
    print("ğŸš€ Iniciando treinamento...")
    model = RandomForestClassifier(**model_params)
    model.fit(X_train, y_train)
    print("âœ… Treinamento concluÃ­do")
    
    # Fazer prediÃ§Ãµes
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # Calcular mÃ©tricas
    train_accuracy = accuracy_score(y_train, y_pred_train)
    test_accuracy = accuracy_score(y_test, y_pred_test)
    precision = precision_score(y_test, y_pred_test, average='weighted')
    recall = recall_score(y_test, y_pred_test, average='weighted')
    f1 = f1_score(y_test, y_pred_test, average='weighted')
    
    print(f"\nğŸ“Š MÃ©tricas do modelo:")
    print(f"   ğŸ¯ AcurÃ¡cia (treino): {train_accuracy:.4f}")
    print(f"   ğŸ¯ AcurÃ¡cia (teste): {test_accuracy:.4f}")
    print(f"   ğŸ¯ Precision: {precision:.4f}")
    print(f"   ğŸ¯ Recall: {recall:.4f}")
    print(f"   ğŸ¯ F1-Score: {f1:.4f}")
    
    # Log parÃ¢metros e mÃ©tricas no MLflow
    try:
        mlflow.log_params(model_params)
        mlflow.log_metric("train_accuracy", train_accuracy)
        mlflow.log_metric("test_accuracy", test_accuracy)
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("f1_score", f1)
        print("âœ… MÃ©tricas registradas no MLflow")
    except Exception as e:
        print(f"âš ï¸ Erro ao registrar mÃ©tricas: {e}")
    
    # Preparar input_example para signature automÃ¡tica
    input_example = X_test.head(3)
    
    # Registrar modelo no MLflow
    try:
        print("ğŸ“ Registrando modelo no MLflow...")
        
        model_info = mlflow.sklearn.log_model(
            sk_model=model,
            artifact_path="model",
            input_example=input_example,
            registered_model_name=MODEL_NAME
        )
        
        print(f"âœ… Modelo registrado: {model_info.model_uri}")
        
        # Tentar promover para Production
        try:
            from mlflow import MlflowClient
            client = MlflowClient()
            
            # Obter a versÃ£o mais recente
            latest_version_info = client.get_latest_versions(MODEL_NAME, stages=["None"])
            if latest_version_info:
                latest_version = latest_version_info[0].version
                
                # Promover para Production
                client.transition_model_version_stage(
                    name=MODEL_NAME,
                    version=latest_version,
                    stage=STAGE
                )
                print(f"âœ… Modelo promovido para stage: {STAGE}")
            
        except Exception as e:
            print(f"âš ï¸ Erro ao promover modelo: {e}")
            print("ğŸ”„ Modelo registrado mas nÃ£o promovido")
            
    except Exception as e:
        print(f"âŒ Erro ao registrar modelo: {e}")
        print("ğŸ”„ Continuando execuÃ§Ã£o sem registro")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‹ RelatÃ³rio de ClassificaÃ§Ã£o

# COMMAND ----------

print("ğŸ“‹ RelatÃ³rio detalhado de classificaÃ§Ã£o:")
print("="*50)
print(classification_report(y_test, y_pred_test))

# COMMAND ----------

# MAGIC %md
# MAGIC ## âœ… Resumo da ExecuÃ§Ã£o

# COMMAND ----------

print("ğŸ‰ TREINAMENTO CONCLUÃDO!")
print("="*50)
print(f"ğŸ“Š Dataset: {len(df)} registros processados")
print(f"ğŸ¤– Modelo: RandomForest treinado")
print(f"ğŸ¯ AcurÃ¡cia final: {test_accuracy:.4f}")
print(f"ğŸ“ Modelo registrado: {MODEL_NAME}")
print(f"ğŸ·ï¸ Stage: {STAGE}")
print("="*50)

# Verificar tabela de resultados
try:
    # Salvar mÃ©tricas em tabela para monitoramento
    metrics_data = {
        'model_name': [MODEL_NAME],
        'training_date': [datetime.now()],
        'accuracy': [test_accuracy],
        'precision': [precision],
        'recall': [recall],
        'f1_score': [f1],
        'input_table': [INPUT_TABLE]
    }
    
    metrics_df = pd.DataFrame(metrics_data)
    metrics_spark_df = spark.createDataFrame(metrics_df)
    
    # Salvar na tabela de estatÃ­sticas
    output_stats_table = "workspace.default.iris_model_stats"
    metrics_spark_df.write \
        .mode("append") \
        .option("mergeSchema", "true") \
        .saveAsTable(output_stats_table)
    
    print(f"ğŸ“Š MÃ©tricas salvas em: {output_stats_table}")
    
except Exception as e:
    print(f"âš ï¸ Erro ao salvar mÃ©tricas: {e}")

print("ğŸš€ Pipeline de treinamento finalizado!")
