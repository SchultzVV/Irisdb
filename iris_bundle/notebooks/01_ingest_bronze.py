# Databricks notebook source
import seaborn as sns
import pandas as pd

# Load Iris dataset via seaborn
df = sns.load_dataset("iris")

# Convert to Spark DataFrame
df_spark = spark.createDataFrame(df)

# Save as managed table in Unity Catalog (avoids DBFS issues)
# This will work in UC-enabled workspaces
output_table = "default.iris_bronze"

try:
    # Try to save as managed table
    df_spark.write.mode("overwrite").option("overwriteSchema", "true").saveAsTable(output_table)
    print(f"‚úÖ Bronze ingestion complete - saved to table: {output_table}")
    
    # Show count to verify
    count = spark.table(output_table).count()
    print(f"‚úÖ Table contains {count} rows")
    
    # üß™ VALIDA√á√ÉO DE QUALIDADE DE DADOS
    print("\nüß™ Executando valida√ß√µes de qualidade de dados...")
    
    try:
        import great_expectations as gx
        from great_expectations.core.batch import RuntimeBatchRequest
        
        # Configurar contexto GE
        context_root_dir = "/Workspace/Users/xultezz@gmail.com/.bundle/iris_bundle/dev/files/great_expectations"
        context = gx.get_context(context_root_dir=context_root_dir)
        
        # Criar batch request para a tabela Bronze
        batch_request = RuntimeBatchRequest(
            datasource_name="iris_data",
            data_connector_name="default_runtime_data_connector_name",
            data_asset_name=output_table,
            runtime_parameters={"query": f"SELECT * FROM {output_table}"},
            batch_identifiers={"default_identifier_name": "bronze_batch"}
        )
        
        # Executar checkpoint de valida√ß√£o
        results = context.run_checkpoint(
            checkpoint_name="iris_bronze_checkpoint",
            validations=[
                {
                    "batch_request": batch_request,
                    "expectation_suite_name": "iris_bronze_suite"
                }
            ]
        )
        
        if results["success"]:
            print("‚úÖ VALIDA√á√ÉO: Todos os testes de qualidade passaram!")
            stats = results.get("statistics", {})
            print(f"üìä Expectativas avaliadas: {stats.get('evaluated_expectations', 'N/A')}")
            print(f"‚úÖ Taxa de sucesso: {stats.get('success_percent', 'N/A')}%")
        else:
            print("‚ùå VALIDA√á√ÉO: Algumas valida√ß√µes falharam!")
            print("üîç Verifique os logs de Great Expectations para detalhes")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Valida√ß√£o Great Expectations falhou: {e}")
        print("üìù Continuando execu√ß√£o sem valida√ß√£o...")
    
except Exception as e:
    print(f"‚ùå Error saving to table: {e}")
    # Fallback: try to save to workspace tmp location
    fallback_path = "/tmp/iris_bronze_data"
    df_spark.write.mode("overwrite").format("parquet").save(fallback_path)
    print(f"‚úÖ Saved to fallback location: {fallback_path}")
